{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import time\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume information in the documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define what documents to load\n",
    "loader = DirectoryLoader(\"./data/\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# interpret information in the documents\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "texts = splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# create and save the local database\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(\"faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a LLM that knows about our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the template we will use when prompting the AI\n",
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# load the language model\n",
    "config = {'max_new_tokens': 1000, 'context_length': 6000, 'temperature': 0.01}\n",
    "llm = CTransformers(model='./model/llama-2-7b-chat.ggmlv3.q2_K.bin',\n",
    "                    model_type='llama', config=config)\n",
    "\n",
    "\n",
    "# load the interpreted information from the local database\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings)\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'])\n",
    "\n",
    "QA_LLM = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={'prompt': prompt})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Questions About our Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model, question):\n",
    "    model_path = model.combine_documents_chain.llm_chain.llm.model\n",
    "    model_name = pathlib.Path(model_path).name\n",
    "    time_start = time.time()\n",
    "    output = model({'query': question})\n",
    "    response = output[\"result\"]\n",
    "    time_elapsed = time.time() - time_start\n",
    "    # display(HTML(f'<code>{model_name} response time: {time_elapsed:.02f} sec</code>'))\n",
    "    display(HTML(f'<strong>Question:</strong> {question}'))\n",
    "    display(HTML(f'<strong>Answer:</strong> {response}'))\n",
    "    # print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(QA_LLM, \"What is equity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(QA_LLM, \"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using with Llama.cpp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mdrafsunsheikh/10784ce9-f796-443d-b973-fe6ae114c687/home/Study/Projects/llama_cpp/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load the interpreted information from the local database\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings)\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "# prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./model/llama-2-7b-chat.gguf.q4_0.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_loader: - kv   2:                        general.description str              = converted from legacy GGJTv3 MOSTLY_Q...\n",
      "llama_model_loader: - kv   3:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv   4:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   8:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.53 GiB (4.50 BPW) \n",
      "llm_load_print_meta: general.name     = llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 3615.75 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 6000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 3000.00 MiB, K (f16): 1500.00 MiB, V (f16): 1500.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 9.61 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    # model_path=\"./llama.cpp/models/quantized_q4_1.gguf\",\n",
    "    model_path=\"./model/llama-2-7b-chat.gguf.q4_0.bin\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=1000,\n",
    "    top_p=1,\n",
    "    n_ctx=6000,\n",
    "    # callback_manager=callback_manager,\n",
    "    # verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: What is equity?\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_LLM = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={'prompt': prompt})\n",
    "\n",
    "def query(model, question):\n",
    "    # model_path = model.combine_documents_chain.llm_chain.llm.model\n",
    "    # model_name = pathlib.Path(model_path).name\n",
    "    time_start = time.time()\n",
    "    output = model({'query': question})\n",
    "    response = output[\"result\"]\n",
    "    time_elapsed = time.time() - time_start\n",
    "    # display(HTML(f'<code>{model_name} response time: {time_elapsed:.02f} sec</code>'))\n",
    "    display(HTML(f'<strong>Question:</strong> {question}'))\n",
    "    display(HTML(f'<strong>Answer:</strong> {response}'))\n",
    "    # print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2554.68 ms\n",
      "llama_print_timings:      sample time =      44.45 ms /    83 runs   (    0.54 ms per token,  1867.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   20551.03 ms /    83 runs   (  247.60 ms per token,     4.04 tokens per second)\n",
      "llama_print_timings:       total time =   20847.50 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> What is equity?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> Equity refers to the fair distribution of resources, opportunities, and benefits among individuals or groups, taking into account their different needs, circumstances, and backgrounds. In the context of education, equity means ensuring that all students have access to quality learning environments and resources, regardless of their socio-economic status, cultural background, or any other factor that may impact their ability to succeed."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(QA_LLM, \"What is equity?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llama_cpp/convert-llama-ggml-to-gguf.py --eps 1e-5 --input model/llama-2-7b-chat.ggmlv3.q4_0.bin --output model/llama-2-7b-chat.gguf.q4_0.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2554.68 ms\n",
      "llama_print_timings:      sample time =      67.76 ms /   118 runs   (    0.57 ms per token,  1741.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4396.41 ms /    23 tokens (  191.15 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:        eval time =   37034.37 ms /   117 runs   (  316.53 ms per token,     3.16 tokens per second)\n",
      "llama_print_timings:       total time =   41915.16 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> hello, how are you?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> We're glad to hear that you're interested in learning more about our commitment to Equity, Diversity, and Inclusion (EDI) at the University of Newcastle! Our EDI vision is centered around creating a fair and inclusive environment for all students and staff, regardless of their background or identity. We believe that everyone has the right to participate, engage, and contribute, and we are committed to advocating for access and inclusion through various action plans and programs. If you have any specific questions or areas of interest, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(QA_LLM, \"hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
