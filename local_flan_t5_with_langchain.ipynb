{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers langchain tiktoken chromadb pypdf InstructorEmbedding accelerate bitsandbytes sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Langchain for Local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mdrafsunsheikh/10784ce9-f796-443d-b973-fe6ae114c687/home/Study/Projects/llama_cpp/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Sequence-to-Sequence Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the directory path where the model is stored on your system\n",
    "model_name = \"../model/flan-t5-large/\"\n",
    "\n",
    "# Initialize a tokenizer for the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize a model for sequence-to-sequence tasks using the specified pretrained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Text-to-Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text-to-text generation using a specified model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",  # Specify the task as text-to-text generation\n",
    "    model=model,              # Use the previously initialized model\n",
    "    tokenizer=tokenizer,      # Use the previously initialized tokenizer\n",
    "    max_length=512,           # Set the maximum length for generated text to 512 tokens\n",
    "    temperature=0,            # Set the temperature parameter for controlling randomness (0 means deterministic)\n",
    "    top_p=0.95,               # Set the top_p parameter for controlling the nucleus sampling (higher values make output more focused)\n",
    "    repetition_penalty=1.15   # Set the repetition_penalty to control the likelihood of repeated words or phrases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Local Language Model for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mdrafsunsheikh/10784ce9-f796-443d-b973-fe6ae114c687/home/Study/Projects/llama_cpp/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/media/mdrafsunsheikh/10784ce9-f796-443d-b973-fe6ae114c687/home/Study/Projects/llama_cpp/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie alte sind Sie?\n"
     ]
    }
   ],
   "source": [
    "# Create a Hugging Face pipeline for local language model (LLM) using the 'pipe' pipeline\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Generate text by providing an input prompt to the LLM pipeline and print the result\n",
    "print(local_llm('translate English to German: How old are you?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Document Retrieval with Local Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents from a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# Create a DirectoryLoader object to load documents from a specified directory\n",
    "loader = DirectoryLoader('./data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "# Load documents from the specified directory using the loader\n",
    "documents = loader.load()\n",
    "\n",
    "# Print the number of loaded documents\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Text into Chunks for Efficient Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RecursiveCharacterTextSplitter object to split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,   # the text will be divided into chunks, and each chunk will contain up to 1000 characters.\n",
    "                                               chunk_overlap=100  # the last 200 characters of one chunk will overlap with the first 200 characters of the next chunk\n",
    "                                               )\n",
    "\n",
    "# Split documents into text chunks using the text splitter\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Hugging Face Instructor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "# Pass the directory path where the embedding model is stored on your system\n",
    "embedding_model_name = \"../model/instructor-base\"\n",
    "\n",
    "# Initialize an instance of HuggingFaceInstructEmbeddings\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"}  # Specify the device to be used for inference (GPU - \"cuda\" or CPU - \"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Storing Texts with Chroma for Future Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "\n",
    "# Define the directory where the embeddings will be stored on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "# Assign the embedding model (instructor_embeddings) to the 'embedding' variable\n",
    "embedding = instructor_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma instance and generate embeddings from the supplied texts\n",
    "# Store the embeddings in the specified 'persist_directory' (on disk)\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
    "\n",
    "# Persist the database (vectordb) to disk\n",
    "vectordb.persist()\n",
    "\n",
    "# Set the vectordb variable to None to release the memory\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Chroma instance by loading the persisted database from the \n",
    "# specified directory and using the provided embedding function.\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the Chroma database (vectordb) with search parameters\n",
    "# The value of \"k\" determines the number of nearest neighbors to retrieve.\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Retrieval-Based Question-Answering System with Local Language Model and Retrieval Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mdrafsunsheikh/10784ce9-f796-443d-b973-fe6ae114c687/home/Study/Projects/llama_cpp/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/media/mdrafsunsheikh/10784ce9-f796-443d-b973-fe6ae114c687/home/Study/Projects/llama_cpp/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How to predict student success factors?\n",
      "Result: Using video learning analytics and data mining techniques\n"
     ]
    }
   ],
   "source": [
    "# Create a Question-Answer (QA) chain for retrieval-based QA using specified components\n",
    "# - 'llm' is the local language model (LLM)\n",
    "# - 'chain_type' specifies the type of QA chain (e.g., \"stuff\")\n",
    "# - 'retriever' is the retrieval component used for finding relevant documents\n",
    "# - 'return_source_documents=True' indicates that source documents will be returned along with the answer.\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=local_llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "# Example query for the QA chain\n",
    "query = \"How to predict student success factors?\"\n",
    "\n",
    "# Use the QA chain to answer the query\n",
    "llm_response = qa_chain(query)\n",
    "\n",
    "# Print the response from the QA chain\n",
    "print(\"Query:\", query)\n",
    "print(\"Result:\",llm_response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
